{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `~/gits/PACMAN_ADworkshop/Day1`\n"
     ]
    }
   ],
   "source": [
    "using Pkg; Pkg.activate(\".\")\n",
    "using ReverseDiff, ForwardDiff, Optimisers, Zygote, \n",
    "      LinearAlgebra, PrettyPrinting, NamedTupleTools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mlp_init"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# define the sigmoid and its derivative. We could of course \n",
    "# evaluate the derivative by hand, but we'll use ForwardDiff.\n",
    "# It is convenient and there is no performance penalty.\n",
    "σ(x) = 1 / (1 + exp(x))\n",
    "dσ(x) = ForwardDiff.derivative(σ, x)\n",
    "\n",
    "\"\"\"\n",
    "Define a simple MLP with 2 hidden layers:\n",
    "\n",
    "   mlp(x) =  W₃ ⋅ σ(W₂ ⋅ σ(W₁ ⋅ x + b₁) + b₂) + b₁\n",
    "\"\"\"\n",
    "function mlp(x, p) \n",
    "   x1 = σ.(p.W1 * x + p.b1)\n",
    "   x2 = σ.(p.W2 * x1 + p.b2)\n",
    "   return dot(p.W3, x2) + p.b3\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "A simple utility function to generate inputs for `mlp`.\n",
    "\"\"\"\n",
    "function mlp_init(Nin, N1, N2) \n",
    "   x = randn(Nin)\n",
    "   p = (W1 = randn(N1, Nin), b1 = randn(N1), \n",
    "        W2 = randn(N2, N1),  b2 = randn(N2), \n",
    "        W3 = randn(N2),      b3 = randn() )\n",
    "   return x, p        \n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.4519502898186545"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# evaluting the mlp gives a scalar output. \n",
    "x, p = mlp_init(3, 4, 2)\n",
    "mlp(x, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "∇x = [0.19589648744066998, 0.13484948834357607, -0.1663725479925786]\n",
      "--------------------------------------------------------------------------------\n",
      "∇p = (W1 =\n",
      "     [0.00890500867210859 -0.00552460882521072 0.005680826470775023; 0.07731720000921377 -0.04796708248576493 0.04932343275916715; -0.002426969567074131 0.0015056759608523473 -0.0015482514917231959; 0.24745625391346748 -0.15352023277706883 0.15786127665360114],\n",
      " b1 = [-0.005576385657806202,\n",
      "       -0.048416631707897305,\n",
      "       0.0015197872101071974,\n",
      "       -0.15495902991981758],\n",
      " W2 =\n",
      "     [-0.17448288553681046 -0.13282264611097902 -0.002058355355426552 -0.16621033460007367; 0.06773809898640472 0.05156467651380961 0.0007990988823122418 0.06452651251763021],\n",
      " b2 = [-0.2597753415777973, 0.1008505089074285],\n",
      " W3 = [0.6913163670183934, 0.06838282040409016],\n",
      " b3 = 1.0)"
     ]
    }
   ],
   "source": [
    "# AD will \"automatically\" give us the gradient \n",
    "# w.r.t. the input and/or w.r.t. the parameters.\n",
    "∇x_ad = Zygote.gradient(_x -> mlp(_x, p), x)[1]\n",
    "∇p_ad = Zygote.gradient(_p -> mlp(x, _p), p)[1]\n",
    "\n",
    "print(\"∇x = \"); pprintln(∇x_ad)\n",
    "println(\"-\"^80)\n",
    "print(\"∇p = \"); pprint(∇p_ad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What AD (Zygote in this case) does is it generate a new function that \n",
    "# first executes the original function (forward pass) but then adds additional \n",
    "# steps - the backward pass - that accumulate the gradient information. \n",
    "# The following function is indicative of how this generated code might look \n",
    "# like if we were to inspect it. \n",
    "\n",
    "function mlp_withgrad(x, p)\n",
    "   # unpack the parameters \n",
    "   W1 = p.W1; b1 = p.b1 \n",
    "   W2 = p.W2; b2 = p.b2 \n",
    "   W3 = p.W3; b3 = p.b3\n",
    "   \n",
    "   # Forward Pass\n",
    "   # fwd stage 1\n",
    "   x1 = σ.(W1 * x + b1)   \n",
    "   # fwd stage 2 \n",
    "   x2 = σ.(W2 * x1 + b2)  \n",
    "   # fwd stage 3 \n",
    "   y = dot(W3, x2) + b3    \n",
    "\n",
    "   # Backward Pass\n",
    "   # bwd stage 3\n",
    "   ∂y_∂x2 = W3\n",
    "   ∂y_∂W3 = x2\n",
    "   ∂y_∂b3 = 1.0\n",
    "   # bwd stage 2\n",
    "   t2 = ∂y_∂x2 .* dσ.(W2 * x1 + b2)  # N₂ - vector\n",
    "   ∂y_∂x1 = W2' * t2\n",
    "   ∂y_∂W2 = t2 * x1'\n",
    "   ∂y_∂b2 = t2 \n",
    "   # bwd stage 1\n",
    "   t1 = ∂y_∂x1 .* dσ.(W1 * x + b1)   # N₁ - vector\n",
    "   ∂y_∂x = W1' * t1\n",
    "   ∂y_∂W1 = t1 * x'\n",
    "   ∂y_∂b1 = t1 \n",
    "\n",
    "   # pack the gradients into a named tuple (~ static Dict)\n",
    "   ∂y_∂p = ( W1 = ∂y_∂W1, b1 = ∂y_∂b1, \n",
    "             W2 = ∂y_∂W2, b2 = ∂y_∂b2, \n",
    "             W3 = ∂y_∂W3, b3 = ∂y_∂b3 )\n",
    "            \n",
    "   return y, ∂y_∂x, ∂y_∂p\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y ≈ mlp(x, p) = true\n",
      "∇x ≈ ∇x_ad = true\n",
      "all((∇p[s] ≈ ∇p_ad[s] for s = fieldnames(∇p))) = true\n"
     ]
    }
   ],
   "source": [
    "# We can evaluate the gradients with our hand-written implementation \n",
    "# and confirm that they are comparable. \n",
    "y, ∇x, ∇p = mlp_withgrad(x, p)\n",
    "@show y ≈ mlp(x, p)\n",
    "@show ∇x ≈ ∇x_ad \n",
    "@show all(∇p[s] ≈ ∇p_ad[s] for s ∈ fieldnames(∇p));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance Zygote: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2.199 μs (53 allocations: 7.16 KiB)\n",
      "Performance Hand-coded: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  635.355 ns (16 allocations: 1.48 KiB)\n"
     ]
    }
   ],
   "source": [
    "# To wrap this up, let's just compare the performance of these \n",
    "# two implementations.\n",
    "using BenchmarkTools\n",
    "\n",
    "println(\"Performance Zygote: \")\n",
    "@btime Zygote.gradient(_p -> mlp($x, _p), $p);\n",
    "\n",
    "println(\"Performance Hand-coded: \")\n",
    "@btime mlp_withgrad($x, $p);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shouldn't read too much into this performance comparison. There are some ways\n",
    "to make Zygote a bit faster and there are MANY ways to make the hand-coded \n",
    "version MUCH faster. \n",
    "\n",
    "But still there is an important message. Backward differentiation \n",
    "is more than just a useful tool for AD. It is also a means for us to organize \n",
    "our algorithms in a systematic way to get the best possible performance. All \n",
    "AD tools have overheads, sometimes they are negligible, sometimes they dominate. \n",
    "Knowing how to hand-code adjoints and chains/networks of function evaluations \n",
    "in a systematic way can sometimes lead to significant performance gains. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.0",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
