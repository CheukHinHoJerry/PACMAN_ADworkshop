{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `~/gits/PACMAN_ADworkshop/Day1`\n"
     ]
    }
   ],
   "source": [
    "using Pkg; Pkg.activate(\".\")\n",
    "using ReverseDiff, ForwardDiff, Optimisers, Zygote, \n",
    "      LinearAlgebra, PrettyPrinting, NamedTupleTools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook implements a very simple multi-layer perceptron (MLP) \n",
    "to illustrate how backpropagation is implemented. \n",
    "\n",
    "The MLP is a function $f : \\mathbb{R}^{N_{\\rm in}} \\to \\mathbb{R}$ defined as \n",
    "$$\n",
    "  f(x) = W_3 \\cdot \\big( W_2 \\sigma( W_1 x+ b_1 ) + b_2 \\big) + b_3\n",
    "$$\n",
    "where $W_1 \\in \\mathbb{R}^{N_1 \\times N_{\\rm in}}, W_2 \\in \\mathbb{R}^{N_2 \\times N_1}, W_3 \\in \\mathbb{R}^{N_2}$ and the $b_i$ analogous. It is more conveniently expressed in stages, \n",
    "$$\n",
    "\\begin{aligned} \n",
    "    x_1 &= W_1 x+ b_1 \\\\ \n",
    "    x_2 &= W_2 x_1 + b_2 \\\\ \n",
    "    f(x) = y &= W_3 \\cdot x_2 + b_3.\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mlp_init"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# define the sigmoid and its derivative. We could of course \n",
    "# evaluate the derivative by hand, but we'll use ForwardDiff.\n",
    "# It is convenient and there is no performance penalty.\n",
    "σ(x) = 1 / (1 + exp(x))\n",
    "dσ(x) = ForwardDiff.derivative(σ, x)\n",
    "\n",
    "\"\"\"\n",
    "Define a simple MLP with 2 hidden layers:\n",
    "\n",
    "   mlp(x) =  W₃ ⋅ σ(W₂ ⋅ σ(W₁ ⋅ x + b₁) + b₂) + b₁\n",
    "\"\"\"\n",
    "function mlp(x, p) \n",
    "   x1 = σ.(p.W1 * x + p.b1)\n",
    "   x2 = σ.(p.W2 * x1 + p.b2)\n",
    "   return dot(p.W3, x2) + p.b3\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "A simple utility function to generate inputs for `mlp`.\n",
    "\"\"\"\n",
    "function mlp_init(Nin, N1, N2) \n",
    "   x = randn(Nin)\n",
    "   p = (W1 = randn(N1, Nin), b1 = randn(N1), \n",
    "        W2 = randn(N2, N1),  b2 = randn(N2), \n",
    "        W3 = randn(N2),      b3 = randn() )\n",
    "   return x, p        \n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0695008358878582"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluting the mlp gives a scalar output. \n",
    "Nin = 3; N1 = 4; N2 = 2\n",
    "x, p = mlp_init(Nin, N1, N2)\n",
    "mlp(x, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "∇x = [0.047955891559071415, -0.021675281122803536, 0.01034124616918743]\n",
      "--------------------------------------------------------------------------------\n",
      "∇p = (W1 =\n",
      "     [0.04528112443269612 0.007323798408162094 -0.010799920402466226; -0.0631973165530103 -0.010221574931492798 0.015073079499968008; 0.008435405462189322 0.0013643479456436618 -0.0020119135444523484; 0.0034269209949327807 0.0005542724223840916 -0.0008173488276737725],\n",
      " b1 = [0.02035796579242188,\n",
      "       -0.0284129165226747,\n",
      "       0.0037924786099273527,\n",
      "       0.0015407113065813935],\n",
      " W2 =\n",
      "     [-0.007362584416253665 -0.05957602220615831 -0.0062375722056526375 -0.0028628691326680776; -0.015267495870337298 -0.12354040668052114 -0.012934603191848998 -0.005936616843921574],\n",
      " b2 = [-0.10026364721322245, -0.20791270201174006],\n",
      " W3 = [0.27098364549387494, 0.2481311529932934],\n",
      " b3 = 1.0)"
     ]
    }
   ],
   "source": [
    "# AD will \"automatically\" give us the gradient \n",
    "# w.r.t. the input and/or w.r.t. the parameters.\n",
    "∇x_ad = Zygote.gradient(_x -> mlp(_x, p), x)[1]\n",
    "∇p_ad = Zygote.gradient(_p -> mlp(x, _p), p)[1]\n",
    "\n",
    "print(\"∇x = \"); pprintln(∇x_ad)\n",
    "println(\"-\"^80)\n",
    "print(\"∇p = \"); pprint(∇p_ad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What AD (Zygote in this case) does is it generates a new function that \n",
    "first executes the original function (forward pass) but then adds additional\n",
    "steps - the backward pass - that accumulate the gradient information\n",
    "The following function is indicative of how this generated code might look\n",
    "like if we were to inspect it. \n",
    "\n",
    "In high level terms it looks like this: \n",
    "$$\n",
    "\\begin{aligned} \n",
    "        \\text{FORWARD PASS} &   \\\\ \n",
    "    x_1 &= f_1(x, W_1, b_1) = W_1 x+ b_1 \\\\ \n",
    "    x_2 &= f_2(x_1, W_2, B_2) = W_2 x_1 + b_2 \\\\ \n",
    "    f(x) = y &= f_3(x_2, W_3, b_3) = W_3 \\cdot x_2 + b_3. \\\\[3mm]\n",
    "    \\text{BACKWARD PASS} &  \\\\ \n",
    "    \\partial_{x_2} f &= \\partial_{x_2} f_3 = W_3 \\\\ \n",
    "    \\partial_{W_3} f &= \\partial_{W_3} f_3 = x_2 \\\\ \n",
    "    \\partial_{b_3} f &= \\partial_{b_3} f_3 = 1  \\\\[2mm] \n",
    "    \\partial_{x_1} f &= \\partial_{x_2} f \\cdot \\partial_{x_1} x_2 \n",
    "                        = \\partial_{x_2} f \\cdot \\partial_{x_1} f_2 = W_2^T \\big(\\partial_{x_2} f \\odot \\sigma'(W_2 x_1 + b_2) \\big)  \\\\ \n",
    "    \\partial_{W_2} f &= \\dots  \\text{(similar calculation)} \\\\ \n",
    "    \\partial_{b_2} f &= \\dots \\\\ \n",
    "    \\partial_{x} f &= \\partial_{x_1} f \\cdot \\partial_{x} x_1 \n",
    "                        = \\partial_{x_1} f \\cdot \\partial_{x} f_1 = W_1^T \\big(\\partial_{x_1} f \\odot \\sigma'(W_1 x + b_1) \\big) \\\\ \n",
    "    \\partial_{W_1} f &= \\dots \\\\ \n",
    "    \\partial_{b_1} f &= \\dots\n",
    "\\end{aligned}\n",
    "$$\n",
    "where $\\odot$ denotes the element-wise multiplication (Hadamard product). After working out all the expressions we can implement this as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "function mlp_withgrad(x, p)\n",
    "   # unpack the parameters \n",
    "   W1 = p.W1; b1 = p.b1 \n",
    "   W2 = p.W2; b2 = p.b2 \n",
    "   W3 = p.W3; b3 = p.b3\n",
    "   \n",
    "   # Forward Pass\n",
    "   # fwd stage 1\n",
    "   z1 = W1 * x + b1\n",
    "   x1 = σ.(z1)   \n",
    "   # fwd stage 2 \n",
    "   z2 = W2 * x1 + b2\n",
    "   x2 = σ.(z2)  \n",
    "   # fwd stage 3 \n",
    "   y = dot(W3, x2) + b3    \n",
    "\n",
    "   # Backward Pass\n",
    "   # bwd stage 3\n",
    "   ∂y_∂x2 = W3\n",
    "   ∂y_∂W3 = x2\n",
    "   ∂y_∂b3 = 1.0\n",
    "   # bwd stage 2\n",
    "   t2 = ∂y_∂x2 .* dσ.(z2)  # N₂ - vector\n",
    "   ∂y_∂x1 = W2' * t2\n",
    "   ∂y_∂W2 = t2 * x1'\n",
    "   ∂y_∂b2 = t2 \n",
    "   # bwd stage 1\n",
    "   t1 = ∂y_∂x1 .* dσ.(z1)   # N₁ - vector\n",
    "   ∂y_∂x = W1' * t1\n",
    "   ∂y_∂W1 = t1 * x'\n",
    "   ∂y_∂b1 = t1 \n",
    "\n",
    "   # pack the gradients into a named tuple (~ static Dict)\n",
    "   ∂y_∂p = ( W1 = ∂y_∂W1, b1 = ∂y_∂b1, \n",
    "             W2 = ∂y_∂W2, b2 = ∂y_∂b2, \n",
    "             W3 = ∂y_∂W3, b3 = ∂y_∂b3 )\n",
    "            \n",
    "   return y, ∂y_∂x, ∂y_∂p\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y ≈ mlp(x, p) = true\n",
      "∇x ≈ ∇x_ad = true\n",
      "all((∇p[s] ≈ ∇p_ad[s] for s = fieldnames(∇p))) = true\n"
     ]
    }
   ],
   "source": [
    "# We can evaluate the gradients with our hand-written implementation \n",
    "# and confirm that they are comparable. \n",
    "y, ∇x, ∇p = mlp_withgrad(x, p)\n",
    "@show y ≈ mlp(x, p)\n",
    "@show ∇x ≈ ∇x_ad \n",
    "@show all(∇p[s] ≈ ∇p_ad[s] for s ∈ fieldnames(∇p));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance Evaluation only: \n",
      "  183.817 ns (6 allocations: 528 bytes)\n",
      "Performance Zygote: \n",
      "  1.762 μs (53 allocations: 7.16 KiB)\n",
      "Performance Hand-coded: \n",
      "  375.810 ns (12 allocations: 1.14 KiB)\n"
     ]
    }
   ],
   "source": [
    "# To wrap this up, let's just compare the performance of these \n",
    "# two implementations.\n",
    "using BenchmarkTools\n",
    "\n",
    "println(\"Performance Evaluation only: \")        \n",
    "@btime mlp($x, $p)\n",
    "\n",
    "println(\"Performance Zygote: \")\n",
    "@btime Zygote.gradient(_p -> mlp($x, _p), $p);\n",
    "\n",
    "println(\"Performance Hand-coded: \")\n",
    "@btime mlp_withgrad($x, $p);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shouldn't read too much into this performance comparison. There are some ways\n",
    "to make Zygote a bit faster and there are MANY ways to make the hand-coded \n",
    "version faster. \n",
    "\n",
    "But still there is an important message. Backward differentiation \n",
    "is more than just a useful tool for AD. It is also a means for us to organize \n",
    "our algorithms in a systematic way to get the best possible performance. All \n",
    "AD tools have overheads, sometimes they are negligible, sometimes they dominate. \n",
    "Knowing how to hand-code adjoints and chains/networks of function evaluations \n",
    "in a systematic way can sometimes lead to significant performance gains. \n",
    "\n",
    "Just to make the point, the following shows a simple trick to improve the \n",
    "performance significantly further. We simply replace all arrays with static \n",
    "arrays for which the size is known at compile time. This moves the arrays \n",
    "onto the stack avoiding all intermediate allocations from system memory. \n",
    "What happens is that our simple hand-written code becomes quite fast, \n",
    "but the AD system overhead prevents the same speedup for the AD.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance Evaluation only with StaticArrays: \n",
      "  31.228 ns (0 allocations: 0 bytes)\n",
      "Performance Zygote with StaticArrays: \n",
      "  750.362 ns (21 allocations: 880 bytes)\n",
      "Performance hand-coded with StaticArrays: \n",
      "  71.709 ns (0 allocations: 0 bytes)\n"
     ]
    }
   ],
   "source": [
    "using StaticArrays\n",
    "\n",
    "x_st = SVector{Nin}(x)\n",
    "p_st = (W1 = SMatrix{N1, Nin}(p.W1), b1 = SVector{N1}(p.b1), \n",
    "        W2 = SMatrix{N2, N1}(p.W2), b2 = SVector{N2}(p.b2), \n",
    "        W3 = SVector{N2}(p.W3), b3 = p.b3 )\n",
    "\n",
    "println(\"Performance Evaluation only with StaticArrays: \")        \n",
    "@btime mlp($x_st, $p_st)\n",
    "\n",
    "println(\"Performance Zygote with StaticArrays: \")\n",
    "@btime Zygote.gradient(_p -> mlp($x_st, _p), $p_st)\n",
    "\n",
    "println(\"Performance hand-coded with StaticArrays: \")\n",
    "@btime mlp_withgrad($x_st, $p_st);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.0",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
